functions:
  - identifier: extract_workflow
    description: Extracts information from the workflow generated by the generator.
    function_name: extract_workflow
  - identifier: static_checker
    description: Checks the static correctness of the workflow.
    function_name: static_checker
  - identifier: vulnerability_scanner
    description: Scans the workflow for vulnerabilities.
    function_name: vulnerability_scanner
  - identifier: retry_increment
    description: Increments the retry counter.
    function_name: retry_increment
  - identifier: extract_judge_score
    description: Extracts the score given by the judge agent.
    function_name: extract_judge_score
agents:
  - identifier: generator
    prompt_template: "{prompt}"
    system_prompt: |
      You are an expert devops engineer. Please generate a YAML file based on the user's input below. No additional explanation is needed. The output format should be ```yaml <Workflow>```.
    tools: [read_readme, read_contributing, list_files, read_file]
  - identifier: corrector
    prompt_template: |
      Description:
        {prompt}
      Workflow:
        {workflow}
      Static analysis results:
        {static_check}
      Identified vulnerabilities:
        {vulnerabilities}
    system_prompt: |
      You are an expert devops engineer. Please correct the YAML file generated by the generator tool. No additional explanation is needed. The output format should be ```yaml <Workflow>```. You may use comments to reason step by step.
    tools: [read_readme, read_contributing, list_files, read_file]
  - identifier: judge
    model: openai/gpt-oss-20b
    system_prompt: |
      You are an expert DevOps engineer. Carefully evaluate whether the provided GitHub Actions workflow accurately and completely implements the requirements described in the accompanying prompt.
    prompt_template: |
      Please use the following Likert scale to rate how well the workflow fulfills the instructions (with 1 being the lowest and 5 being the highest):

      1. Strongly Disagree – The workflow does not follow the instructions at all.
      2. Disagree – The workflow follows the instructions in only a few aspects, with major omissions or errors.
      3. Neutral – The workflow partially follows the instructions, but there are significant places where it falls short or deviates.
      4. Agree – The workflow generally follows the instructions, with only minor issues or omissions.
      5. Strongly Agree – The workflow fully and accurately implements all requirements described in the prompt.

      **Instructions:**
      - Do **not** allow the length, formatting, or verbosity of the response to affect your judgment.
      - Assess only the accuracy and completeness of implementation relative to the prompt's requirements.
      - First, clearly explain your reasoning, referencing specific aspects of the workflow and prompt as needed.

      Then, conclude with your rating in the following format: Therefore, I would rate the workflow with a score of **X out of 5**.

      Here's the description of the workflow:
        {prompt}

      Here's the workflow:
        {workflow}
    tools: [read_readme, read_contributing, list_files, read_file]

routers:
  - identifier: validity_router
    function_name: validity_router

edges:
  - source: START
    target: generator
  - source: generator
    target: extract_workflow
  - source: extract_workflow
    target: static_checker
  - source: static_checker
    target: syntax_corrector
  - source: syntax_corrector
    target: syntax_corrector
  - source: judge
    target: extract_judge_score
  - source: extract_judge_score
    target: static_checker
  - source: vulnerability_scanner
    router: validity_router
    targets:
      valid: END
      invalid: retry_increment
  - source: retry_increment
    target: corrector
  - source: corrector
    target: extract_workflow
